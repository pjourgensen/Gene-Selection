{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pandas: {}'.format(pd.__version__))\n",
    "print('Numpy: {}'.format(np.__version__))\n",
    "print('Matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('Tensorflow: {}'.format(tf.__version__))\n",
    "print('SKLearn: {}'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv('data_set_ALL_AML_train.csv'),\n",
    "                pd.read_csv('data_set_ALL_AML_independent.csv')],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean & Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = [i for i in df.columns if i[:4] != 'call']\n",
    "df = df.loc[:,patients]\n",
    "gene_description = df['Gene Description']\n",
    "df.drop('Gene Description',axis=1,inplace=True)\n",
    "df.set_index(df.iloc[:,0],inplace=True)\n",
    "df.drop('Gene Accession Number',axis=1,inplace=True)\n",
    "df = df.transpose()\n",
    "df.index = df.index.astype('int64')\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv('actual.csv')\n",
    "target.set_index('patient',inplace=True)\n",
    "target.index = target.index.astype('int64')\n",
    "target.sort_index(inplace=True)\n",
    "del target.index.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode target variable\n",
    "\n",
    "def impute_target(x):\n",
    "    if x == 'ALL':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target['One-hot'] = target['cancer'].apply(lambda x: impute_target(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train and test sets\n",
    "\n",
    "train = df.iloc[:38,:]\n",
    "test = df.iloc[38:,:]\n",
    "\n",
    "target_train = target.iloc[:38,1]\n",
    "target_test = target.iloc[38:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return subset of data and corresponding target values\n",
    "\n",
    "def subset(rows,columns,data,target):\n",
    "    return data.iloc[rows,columns], target.iloc[rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set size of subset (num_rows x num_cols)\n",
    "num_rows = 30\n",
    "num_cols = 50\n",
    "\n",
    "train_rows = len(train)\n",
    "train_cols = len(train.columns)\n",
    "\n",
    "test_rows = len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load or initialize genes, predictions, and accuracies lists\n",
    "\n",
    "#with open(\"genes.txt\", \"rb\") as fp:\n",
    "#    genes = pickle.load(fp)\n",
    "#\n",
    "#with open(\"predictions.txt\", \"rb\") as fp:\n",
    "#    predictions = pickle.load(fp)\n",
    "#\n",
    "#with open(\"accuracies.txt\", \"rb\") as fp:\n",
    "#    accuracies = pickle.load(fp)\n",
    "#\n",
    "#\n",
    "genes = []\n",
    "predictions = []\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set number of models to train\n",
    "reps = 1\n",
    "for iters in range(reps): \n",
    "    #Suppress output except for error messages\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    #Select Rows and Cols\n",
    "    rows = random.sample(range(train_rows),num_rows)\n",
    "    cols = random.sample(range(train_cols),num_cols)\n",
    "\n",
    "    #Save Genes used for model\n",
    "    genes.append(train.columns[cols])\n",
    "\n",
    "    #feature columns\n",
    "    feature_columns = []\n",
    "    for i in train.columns[cols]:\n",
    "        feature_columns.append(tf.feature_column.numeric_column(i))\n",
    "\n",
    "    #Model Initialization\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        hidden_units=[25, 10, 5],\n",
    "        optimizer=tf.train.AdamOptimizer(1e-2),\n",
    "        n_classes=2,\n",
    "        dropout=0.2,\n",
    "    )\n",
    "\n",
    "    #Input Function\n",
    "    train_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "        x=subset(rows,cols,train,target_train)[0],\n",
    "        y=subset(rows,cols,train,target_train)[1],\n",
    "        num_epochs=10,\n",
    "        batch_size=num_rows,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    #model training\n",
    "    classifier.train(input_fn=train_input_fn, steps=1000)\n",
    "\n",
    "    #eval function\n",
    "    test_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "        x=test.iloc[:,cols],\n",
    "        y=target_test.iloc[:],\n",
    "        num_epochs=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    #predictions\n",
    "    preds = list(classifier.predict(test_input_fn))\n",
    "\n",
    "    pred_class = [p[\"classes\"] for p in preds]\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for i in range(len(pred_class)):\n",
    "        preds.append(int(pred_class[i][0]))\n",
    "\n",
    "    predictions.append(preds)\n",
    "\n",
    "    accuracies.append(accuracy_score(target_test,preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store updated lists\n",
    "\n",
    "#with open('genes.txt', 'wb') as fp:\n",
    "#    pickle.dump(genes, fp)\n",
    "#\n",
    "#with open('predictions.txt', 'wb') as fp:\n",
    "#    pickle.dump(predictions, fp)\n",
    "#    \n",
    "#with open('accuracies.txt', 'wb') as fp:\n",
    "#    pickle.dump(accuracies, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Predictions to -1/+1\n",
    "for i in range(len(predictions)):\n",
    "    for j in range(len(predictions[i])):\n",
    "        if predictions[i][j] == 0:\n",
    "            predictions[i][j] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Target Test Final Values\n",
    "target_final = [-1 if i==0 else i for i in target_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in a matrix of predictions, multiples them by their corresponding weight and averages the sum across all \n",
    "#models to form the ensembled prediction.\n",
    "def final_preds(pred_list,weights):\n",
    "    final_preds = []\n",
    "    for i in range(len(pred_list[0])):\n",
    "        pred_sum = 0\n",
    "        for j in range(len(pred_list)):\n",
    "            pred_sum += pred_list[j][i]*weights[j]\n",
    "        if pred_sum == 0:\n",
    "            final_preds.append(pred_sum)\n",
    "        else:\n",
    "            final_preds.append(pred_sum/abs(pred_sum))\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give every model equal weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Every model is treated equally\n",
    "weights1 = [1] * len(accuracies)\n",
    "\n",
    "#Final Predictions\n",
    "final_preds1 = final_preds(predictions,weights1)\n",
    "\n",
    "#Accuracy\n",
    "accuracy_score(target_final,final_preds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight each model by its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Every model is weighted by its accuracy\n",
    "final_preds2 = final_preds(predictions,accuracies)\n",
    "\n",
    "#Accuracy\n",
    "accuracy_score(target_final,final_preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model weight equals max{0, accuracy - mode(accuracy)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cut off all models that predicted majority class only or performed worse\n",
    "mode = max(set(accuracies), key=accuracies.count)\n",
    "weights3 = []\n",
    "\n",
    "for i in accuracies:\n",
    "    if i <= mode:\n",
    "        weights3.append(0)\n",
    "    else:\n",
    "        weights3.append(i-mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight each model by above logic\n",
    "final_preds3 = final_preds(predictions,weights3)\n",
    "\n",
    "#Accuracy\n",
    "accuracy_score(target_final,final_preds3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass above weights through exponential function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply exponential to weights3\n",
    "\n",
    "weights4 = []\n",
    "\n",
    "for i in weights3:\n",
    "    weights4.append(np.exp(i)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight each model by scaled weight\n",
    "final_preds4 = final_preds(predictions,weights4)\n",
    "\n",
    "#Accuracy\n",
    "accuracy_score(target_final,final_preds4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ensemble accuracy as a function of the number of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns ensembled prediction accuracy as a function of the number of models\n",
    "def ensemble(predictions,weights,target):\n",
    "    acc_list = []\n",
    "    for i in range(len(predictions)):\n",
    "        ens_preds = final_preds(predictions[:(i+1)],weights[:(i+1)])\n",
    "        acc_list.append(accuracy_score(target,ens_preds))\n",
    "    return acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list1 = ensemble(predictions,weights1,target_final)\n",
    "acc_list2 = ensemble(predictions,accuracies,target_final)\n",
    "acc_list3 = ensemble(predictions,weights3,target_final)\n",
    "acc_list4 = ensemble(predictions,weights4,target_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,8))\n",
    "\n",
    "plt.plot(acc_list1,color='red',label='Equal')\n",
    "plt.plot(acc_list2,'b--',label='Accuracy')\n",
    "plt.plot(acc_list3,color='green',label='ReLu_Lin')\n",
    "plt.plot(acc_list4,color='yellow',label='ReLu_Exp')\n",
    "\n",
    "plt.title('Accuracy vs. # of Models')\n",
    "plt.xlabel('# of Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(bottom=0.4)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genes = len(df.columns)\n",
    "\n",
    "gene_ave = pd.Series(data=np.zeros(num_genes),index=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If a gene is used in a model, add that model's weight to the gene's score and average over number of occurrences\n",
    "\n",
    "for i in gene_ave.index:\n",
    "    count = 0\n",
    "    for j in range(len(genes)):\n",
    "        if i in genes[j]:\n",
    "            gene_ave[i] = ((gene_ave[i]*count) + weights4[j]) / (count + 1)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifers of Top 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get indices of 50 highest scoring genes\n",
    "top50 = gene_ave.nlargest(50).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_pipeline(n):\n",
    "    top = gene_ave.nlargest(n).index\n",
    "    train_set = train.loc[:,top]\n",
    "    test_set = test.loc[:,top]\n",
    "    \n",
    "    knn_acc = []\n",
    "    for i in range(n):\n",
    "        KNN_model = KNeighborsClassifier(n_neighbors=i+1)\n",
    "        cross_val = cross_val_score(KNN_model,train_set,target_train,cv=5)\n",
    "        knn_acc.append(cross_val.mean())\n",
    "    \n",
    "    return knn_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_acc1 = knn_pipeline(1)\n",
    "KNN_acc3 = knn_pipeline(3)\n",
    "KNN_acc5 = knn_pipeline(5)\n",
    "KNN_acc10 = knn_pipeline(10)\n",
    "#KNN_acc50 = knn_pipeline(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.plot(KNN_acc1, color='red',marker='o', label='1')\n",
    "plt.plot(KNN_acc3, color='blue',marker='o', label='3')\n",
    "plt.plot(KNN_acc5, color='yellow',marker='o', label='5')\n",
    "plt.plot(KNN_acc10, color='green',marker='o', label='10')\n",
    "#plt.plot(KNN_acc50, color='orange',marker='o', label='50')\n",
    "\n",
    "plt.title('Accuracy vs. # of Neighbors')\n",
    "plt.xlabel('# of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(bottom=0.825)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats on genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_vals = []\n",
    "\n",
    "for i in gene_ave.nlargest(10).index:\n",
    "    weight_arr = []\n",
    "    for j in range(len(genes)):\n",
    "        if i in genes[j]:\n",
    "            weight_arr.append(weights4[j])\n",
    "    p = stats.ttest_1samp(weight_arr,0)[1]\n",
    "    p_vals.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
